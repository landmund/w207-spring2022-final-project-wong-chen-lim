{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Whales and Dolphin ID\n\n\n","metadata":{}},{"cell_type":"code","source":"import gc\nimport glob\nimport os\nfrom numpy import save, load\nfrom random import seed\nfrom os import listdir\nfrom shutil import copyfile\nfrom random import seed\nfrom random import random\n\nimport joblib\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom contextlib import contextmanager\nfrom multiprocessing.pool import ThreadPool, Pool\nfrom joblib import Parallel, delayed\nimport time\n\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom skimage.color import rgb2gray, gray2rgb\nimport matplotlib.pyplot as plt\n\nseed(2022)\nCPUS = 4 # kaggle default - https://www.kaggle.com/product-feedback/64106\nHAPPYWHALE_INPUT_DIR = \"/kaggle/input/happy-whale-and-dolphin\"\n# these are paramters to determine how to sample the initial training dataset\nMAX_INDIVIDUALS_PER_SPECIES = os.environ.get('MAX_INDIVIDUALS_PER_SPECIES', None) # none to include all\nMAX_IMAGES_PER_INDIVIDUAL_THRESHOLD = os.environ.get('MAX_IMAGES_PER_INDIVIDUAL_THRESHOLD', 0) # 0 to include all\nMAX_SAMPLE_DATA_SIZE = 37000 # change this to determine max images to include in training\nWORKING_DIR = \"/kaggle/working\"\nHEIGHT = 256\nWIDTH = 256\nVALIDATION_RATIO = .25 # used to split training and validation dataset\nIMARRAY_SIZE = (HEIGHT,WIDTH)\n\nEPOCHS = 10\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-04-09T06:33:04.730643Z","iopub.execute_input":"2022-04-09T06:33:04.730968Z","iopub.status.idle":"2022-04-09T06:33:04.747734Z","shell.execute_reply.started":"2022-04-09T06:33:04.730937Z","shell.execute_reply":"2022-04-09T06:33:04.746975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef convert_image_to_ndarray(img_path, output_size=IMARRAY_SIZE, color='normalize_gray'):\n    '''\n    reads the `img_path` and converts it to a numpy array of size `output_size`\n    Supported Options for `color`:\n    - 'normalize_gray'\n    - 'raw_rgb'\n    \n    Returns the \n    1. normalized and standardized image\n    2. aspect ratio of the original image\n    '''\n    # read input\n    im = imread(img_path)\n    if output_size == None:\n        return im\n    aspect_ratio = float(im.shape[1] / im.shape[0])\n    resized_im = resize(im, output_size)\n    if color == 'raw_rgb':\n        return resized_im, aspect_ratio\n    elif color == 'normalize_gray':\n        # if image only has 2 channels, it's already grayscale\n        if len(resized_im.shape) == 2:\n            return resized_im / np.max(resized_im), aspect_ratio\n        # if image has 3 channels. it's rgb and needs to be converted to grayscale\n        elif len(resized_im.shape) == 3:\n            gray_img = rgb2gray(resized_im)\n            # Now normalize gray image\n            gray_norm = gray_img / np.max(gray_img)\n            return gray_norm, aspect_ratio\n        \ndef convert_ndarray_img_to_maintain_aspect_ratio(ndarray_img, ar):\n    '''\n    aspect ratio is width / height\n    '''\n    new_width = ndarray_img.shape[1]*ar # new width should be aspect ratio * img width\n    new_ar_shape = (ndarray_img.shape[0], new_width)\n    return resize(ndarray_img, new_ar_shape)\n\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield\n    print('{0} done in {1:.3f} seconds.'.format(name, time.time() - t0))\n\ndef split_df(df, num_splits, log=False):\n    \n    df_list = []\n    rows_splits = np.linspace(0, df.shape[0], num_splits+1).astype(int)\n    if log:\n        print('Split into {} parts'.format(num_splits))\n        print('Row splits:\\n{}'.format(rows_splits))\n    \n    for i in range(len(rows_splits) - 1):\n        df_list.append(df.iloc[rows_splits[i]:rows_splits[i+1]])\n        \n    return df_list[:num_splits]\n\ndef split_arr(arr, num_splits, log=False):\n    rows_splits = np.linspace(0, df.shape[0], num_splits+1).astype(int)\n    if log:\n        print('Split into {} parts'.format(num_splits))\n        print('Row splits:\\n{}'.format(rows_splits))\n    \n    r_list = []\n    for i in range(len(rows_splits) - 1):\n        r_list.append(arr[rows_splits[i]:rows_splits[i+1]])\n        \n    return r_list[:num_splits]\n\n# create lambda function to construct the full path for a given image.\n# this is intended to be used on the `image` column in train.csv\nget_img_path = lambda img: os.path.join(HAPPYWHALE_INPUT_DIR, img)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-09T06:33:06.173177Z","iopub.execute_input":"2022-04-09T06:33:06.17422Z","iopub.status.idle":"2022-04-09T06:33:06.192094Z","shell.execute_reply.started":"2022-04-09T06:33:06.174177Z","shell.execute_reply":"2022-04-09T06:33:06.190843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sample data\n\nSample / Select images to be used for training in the challengeâ€™s `train.csv`","metadata":{}},{"cell_type":"code","source":"\ntrain_csv_fn = \"train.csv\"\n# order by image jpg file and reset index to have deterministic index\ntrain_df = pd.read_csv(os.path.join(HAPPYWHALE_INPUT_DIR, train_csv_fn)).sort_values('image').reset_index()\n\ntrain_df.species.replace({\"globis\": \"short_finned_pilot_whale\",\n                          \"pilot_whale\": \"short_finned_pilot_whale\",\n                          \"kiler_whale\": \"killer_whale\",\n                          \"bottlenose_dolpin\": \"bottlenose_dolphin\"}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T06:33:08.177285Z","iopub.execute_input":"2022-04-09T06:33:08.177813Z","iopub.status.idle":"2022-04-09T06:33:08.316722Z","shell.execute_reply.started":"2022-04-09T06:33:08.177777Z","shell.execute_reply":"2022-04-09T06:33:08.315578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwith timer('Sampling data: '):\n    individual_ids_by_species = pd.DataFrame(train_df[['species', 'individual_id']].value_counts()).rename({0:'count'}, axis=1)\n    subsetted_individuals = individual_ids_by_species[individual_ids_by_species['count']>MAX_IMAGES_PER_INDIVIDUAL_THRESHOLD].reset_index()#['count'].sum()\n    sampled_data = []\n    for iid in subsetted_individuals.individual_id:\n        images_of_iid = train_df[train_df['individual_id']==iid]\n        sample = images_of_iid[:MAX_INDIVIDUALS_PER_SPECIES]\n        sampled_data.append(sample)\n    sampled_data_df = pd.concat(sampled_data)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T06:34:14.947049Z","iopub.execute_input":"2022-04-09T06:34:14.947442Z","iopub.status.idle":"2022-04-09T06:35:48.209075Z","shell.execute_reply.started":"2022-04-09T06:34:14.947396Z","shell.execute_reply":"2022-04-09T06:35:48.207568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(sampled_data_df['individual_id'].unique())","metadata":{"execution":{"iopub.status.busy":"2022-04-09T06:31:03.794803Z","iopub.status.idle":"2022-04-09T06:31:03.795443Z","shell.execute_reply.started":"2022-04-09T06:31:03.795182Z","shell.execute_reply":"2022-04-09T06:31:03.795209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-04-09T06:35:50.557065Z","iopub.execute_input":"2022-04-09T06:35:50.557404Z","iopub.status.idle":"2022-04-09T06:35:50.562607Z","shell.execute_reply.started":"2022-04-09T06:35:50.557367Z","shell.execute_reply":"2022-04-09T06:35:50.561398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(pd.DataFrame(train_df[['species', 'individual_id']].value_counts()).rename({0:'count'}, axis=1))","metadata":{"execution":{"iopub.status.busy":"2022-04-09T06:47:35.840776Z","iopub.execute_input":"2022-04-09T06:47:35.842297Z","iopub.status.idle":"2022-04-09T06:47:50.740429Z","shell.execute_reply.started":"2022-04-09T06:47:35.84223Z","shell.execute_reply":"2022-04-09T06:47:50.739715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c = pd.DataFrame(train_df[['species', 'individual_id']].value_counts()).rename({0:'count'}, axis=1).reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T06:49:34.834154Z","iopub.execute_input":"2022-04-09T06:49:34.83515Z","iopub.status.idle":"2022-04-09T06:49:34.890602Z","shell.execute_reply.started":"2022-04-09T06:49:34.835103Z","shell.execute_reply":"2022-04-09T06:49:34.889865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sns.barplot(c['individual_id'], c['count'])\nc['count'].describe()\nc\nsns.histplot(c['count'][:int(.25*15587)])","metadata":{"execution":{"iopub.status.busy":"2022-04-09T06:53:54.774758Z","iopub.execute_input":"2022-04-09T06:53:54.775021Z","iopub.status.idle":"2022-04-09T06:53:56.290769Z","shell.execute_reply.started":"2022-04-09T06:53:54.774993Z","shell.execute_reply":"2022-04-09T06:53:56.289756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set up directories and datasets\n\nSet up the working directory such that folder names correspond to species class and contents are the images of that species.","metadata":{}},{"cell_type":"code","source":"def setup_directories(unique_species, log=False):\n    subdirs = ['train', 'test']\n    for subdir in subdirs:\n        for species in unique_species:\n            # create label subdirectories\n            newdir = \"{}/{}/{}\".format(WORKING_DIR, subdir, species)\n            if log:\n                print('Creating: ', newdir)\n            os.makedirs(newdir, exist_ok=True)\n\ndef setup_datasets(df, val_ratio=VALIDATION_RATIO, file_type=\".jpg\", log=False):\n    '''\n    Set up the working directory such that\n    folder names correspond to species class and\n    contents are the images of that species.\n    '''\n    num_saved_files = 0\n    for index, row in df.iterrows():\n        label = row['species']\n        image = row['image']\n        src_img_path = get_img_path('train_images/{}'.format(image))\n        output_fn = image.split('.')[0] + file_type\n        dst_img_path = \"{working_dir}/{dataset}/{label}/{filename}\".format(working_dir=WORKING_DIR,\n                                                                              dataset='test' if random() < val_ratio else 'train',\n                                                                              label=label,\n                                                                              filename=output_fn,\n                                                                              file_type=file_type\n                                                                             )\n        try:\n            if log:\n                    print('Saving {} to {} as {}'.format(src_img_path,\n                                                         dst_img_path,\n                                                         type(arr)))\n            if file_type == '.npy':\n                arr, ar = convert_image_to_ndarray(src_img_path, color='raw_rgb')\n                save(dst_img_path, arr)\n            elif file_type == '.jpg':\n                copyfile(src_img_path,dst_img_path)\n                \n            num_saved_files +=1\n        except Exception as e:\n            print(type(e))\n            print(e)\n            print('Skipping: ', src_img_path)\n    return num_saved_files\n\n\ndef pool_setup_datasets(df, timer_str, CPUS=4, log=False, max_images_to_include=MAX_SAMPLE_DATA_SIZE):\n    df_splits = split_df(df[:max_images_to_include], num_splits=CPUS, log=log)\n    with timer(timer_str):\n        with Pool(processes=CPUS) as pool:\n            num_files_saved = pool.map(setup_datasets, df_splits)\n    total_saved = sum(num_files_saved)\n    if log:\n        print('Saved {} image arrays or image files'.format(total_saved))\n    return total_saved\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-08T18:19:33.580945Z","iopub.execute_input":"2022-04-08T18:19:33.581353Z","iopub.status.idle":"2022-04-08T18:19:33.597116Z","shell.execute_reply.started":"2022-04-08T18:19:33.581314Z","shell.execute_reply":"2022-04-08T18:19:33.596237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_ids = sorted(sampled_data_df['species'].unique())\n\nsetup_directories(unique_ids, log=False)\npool_setup_datasets(sampled_data_df, timer_str='setting up datasets', log=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-08T00:35:16.167948Z","iopub.execute_input":"2022-04-08T00:35:16.168616Z","iopub.status.idle":"2022-04-08T00:35:16.57812Z","shell.execute_reply.started":"2022-04-08T00:35:16.168565Z","shell.execute_reply":"2022-04-08T00:35:16.577418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the Xception CNN model.\n\n\nReferences:\n\n- https://arxiv.org/pdf/1610.02357.pdf\n- https://github.com/otenim/Xception-with-Your-Own-Dataset\n","metadata":{}},{"cell_type":"code","source":"import math\nimport os\nimport argparse\nimport matplotlib\nimport imghdr\nimport pickle as pkl\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.applications.xception import Xception, preprocess_input\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.preprocessing import image\nfrom keras.losses import categorical_crossentropy\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras.models import Model\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint\n\nmatplotlib.use('Agg')\n\ndef generate_from_paths_and_labels(\n        input_paths, labels, batch_size, input_size=(299, 299)):\n    num_samples = len(input_paths)\n    while 1:\n        perm = np.random.permutation(num_samples)\n        input_paths = input_paths[perm]\n        labels = labels[perm]\n        for i in range(0, num_samples, batch_size):\n            inputs = list(map(\n                lambda x: image.load_img(x, target_size=input_size),\n                input_paths[i:i+batch_size]\n            ))\n            inputs = np.array(list(map(\n                lambda x: image.img_to_array(x),\n                inputs\n            )))\n            inputs = preprocess_input(inputs)\n            yield (inputs, labels[i:i+batch_size])\n\n\ndef fine_tune(dataset_root,\n                classes,\n                result_root,\n                epochs_pre=5,\n                epochs_fine=EPOCHS,\n                batch_size_pre=32,\n                batch_size_fine=16,\n                lr_pre=1e-3,\n                lr_fine=1e-4,\n                snapshot_period_pre=1,\n                snapshot_period_fine=1,\n                split=0.8,):\n    '''\n    Reference https://github.com/otenim/Xception-with-Your-Own-Dataset\n    '''\n\n    # ====================================================\n    # Preparation\n    # ====================================================\n    # parameters\n    epochs = epochs_pre + epochs_fine\n    dataset_root = os.path.expanduser(dataset_root)\n    result_root = os.path.expanduser(result_root)\n\n    # load class names\n#     with open(classes, 'r') as f:\n#         classes = f.readlines()\n#         classes = list(map(lambda x: x.strip(), classes))\n    num_classes = len(classes)\n\n    # make input_paths and labels\n    input_paths, labels = [], []\n    for class_name in os.listdir(dataset_root):\n        class_root = os.path.join(dataset_root, class_name)\n        class_id = classes.index(class_name)\n        for path in os.listdir(class_root):\n            path = os.path.join(class_root, path)\n            if imghdr.what(path) is None:\n                # this is not an image file\n                continue\n            input_paths.append(path)\n            labels.append(class_id)\n\n    # convert to one-hot-vector format\n    labels = to_categorical(labels, num_classes=num_classes)\n\n    # convert to numpy array\n    input_paths = np.array(input_paths)\n\n    # shuffle dataset\n    perm = np.random.permutation(len(input_paths))\n    labels = labels[perm]\n    input_paths = input_paths[perm]\n\n    # split dataset for training and validation\n    border = int(len(input_paths) * split)\n    train_labels = labels[:border]\n    val_labels = labels[border:]\n    train_input_paths = input_paths[:border]\n    val_input_paths = input_paths[border:]\n    print(\"Training on %d images and labels\" % (len(train_input_paths)))\n    print(\"Validation on %d images and labels\" % (len(val_input_paths)))\n\n    # create a directory where results will be saved (if necessary)\n    if os.path.exists(result_root) is False:\n        os.makedirs(result_root)\n\n    # ====================================================\n    # Build a custom Xception\n    # ====================================================\n    # instantiate pre-trained Xception model\n    # the default input shape is (299, 299, 3)\n    # NOTE: the top classifier is not included\n    base_model = Xception(\n        include_top=False,\n        weights='imagenet',\n        input_shape=(HEIGHT, WIDTH, 3))\n\n    # create a custom top classifier\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024, activation='relu')(x)\n    predictions = Dense(num_classes, activation='softmax')(x)\n    model = Model(inputs=base_model.inputs, outputs=predictions)\n\n    # ====================================================\n    # Train only the top classifier\n    # ====================================================\n    # freeze the body layers\n    for layer in base_model.layers:\n        layer.trainable = False\n\n    # compile model\n    model.compile(\n        loss=categorical_crossentropy,\n        optimizer=Adam(lr=lr_pre),\n        metrics=['accuracy']\n    )\n\n    # train\n    hist_pre = model.fit_generator(\n        generator=generate_from_paths_and_labels(\n            input_paths=train_input_paths,\n            labels=train_labels,\n            batch_size=batch_size_pre\n        ),\n        steps_per_epoch=math.ceil(\n            len(train_input_paths) / batch_size_pre),\n        epochs=epochs_pre,\n        validation_data=generate_from_paths_and_labels(\n            input_paths=val_input_paths,\n            labels=val_labels,\n            batch_size=batch_size_pre\n        ),\n        validation_steps=math.ceil(\n            len(val_input_paths) / batch_size_pre),\n        verbose=1,\n        callbacks=[\n            ModelCheckpoint(\n                filepath=os.path.join(\n                    result_root,\n                    'model_pre_ep{epoch}_valloss{val_loss:.3f}.h5'),\n                period=snapshot_period_pre,\n            ),\n        ],\n    )\n    model.save(os.path.join(result_root, 'model_pre_final.h5'))\n\n    # ====================================================\n    # Train the whole model\n    # ====================================================\n    # set all the layers to be trainable\n    for layer in model.layers:\n        layer.trainable = True\n\n    # recompile\n    model.compile(\n        optimizer=Adam(lr=lr_fine),\n        loss=categorical_crossentropy,\n        metrics=['accuracy'])\n\n    # train\n    hist_fine = model.fit_generator(\n        generator=generate_from_paths_and_labels(\n            input_paths=train_input_paths,\n            labels=train_labels,\n            batch_size=batch_size_fine\n        ),\n        steps_per_epoch=math.ceil(\n            len(train_input_paths) / batch_size_fine),\n        epochs=epochs_fine,\n        validation_data=generate_from_paths_and_labels(\n            input_paths=val_input_paths,\n            labels=val_labels,\n            batch_size=batch_size_fine\n        ),\n        validation_steps=math.ceil(\n            len(val_input_paths) / batch_size_fine),\n        verbose=1,\n        callbacks=[\n            ModelCheckpoint(\n                filepath=os.path.join(\n                    result_root,\n                    'model_fine_ep{epoch}_valloss{val_loss:.3f}.h5'),\n                period=snapshot_period_fine,\n            ),\n        ],\n    )\n    model.save(os.path.join(result_root, 'model_fine_final.h5'))\n\n    # ====================================================\n    # Create & save result graphs\n    # ====================================================\n    # concatinate plot data\n    acc = hist_pre.history['accuracy']\n    val_acc = hist_pre.history['val_accuracy']\n    loss = hist_pre.history['loss']\n    val_loss = hist_pre.history['val_loss']\n    acc.extend(hist_fine.history['accuracy'])\n    val_acc.extend(hist_fine.history['val_accuracy'])\n    loss.extend(hist_fine.history['loss'])\n    val_loss.extend(hist_fine.history['val_loss'])\n\n    # save graph image\n    plt.plot(range(epochs), acc, marker='.', label='accuracy')\n    plt.plot(range(epochs), val_acc, marker='.', label='val_accuracy')\n    plt.legend(loc='best')\n    plt.grid()\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.savefig(os.path.join(result_root, 'accuracy.png'))\n    plt.clf()\n\n    plt.plot(range(epochs), loss, marker='.', label='loss')\n    plt.plot(range(epochs), val_loss, marker='.', label='val_loss')\n    plt.legend(loc='best')\n    plt.grid()\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.savefig(os.path.join(result_root, 'loss.png'))\n    plt.clf()\n\n    # save plot data as pickle file\n    plot = {\n        'accuracy': acc,\n        'val_accuracy': val_acc,\n        'loss': loss,\n        'val_loss': val_loss,\n    }\n    with open(os.path.join(result_root, 'plot.dump'), 'wb') as f:\n        pkl.dump(plot, f)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-08T00:35:18.688889Z","iopub.execute_input":"2022-04-08T00:35:18.689697Z","iopub.status.idle":"2022-04-08T00:35:24.635677Z","shell.execute_reply.started":"2022-04-08T00:35:18.689648Z","shell.execute_reply":"2022-04-08T00:35:24.634824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fine_tune('{}/train/'.format(WORKING_DIR), unique_ids, WORKING_DIR)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T23:32:38.829801Z","iopub.execute_input":"2022-04-07T23:32:38.830622Z","iopub.status.idle":"2022-04-08T00:29:55.310369Z","shell.execute_reply.started":"2022-04-07T23:32:38.830573Z","shell.execute_reply":"2022-04-08T00:29:55.308109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}