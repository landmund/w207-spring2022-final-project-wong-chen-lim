{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    print(dirname)\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-07T02:22:05.190067Z","iopub.execute_input":"2022-04-07T02:22:05.19067Z","iopub.status.idle":"2022-04-07T02:22:11.515854Z","shell.execute_reply.started":"2022-04-07T02:22:05.19055Z","shell.execute_reply":"2022-04-07T02:22:11.514568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport glob\nimport os\n\nimport joblib\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom contextlib import contextmanager\nfrom multiprocessing.pool import ThreadPool, Pool\nfrom joblib import Parallel, delayed\nimport time\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-03-12T08:39:29.862337Z","iopub.execute_input":"2022-03-12T08:39:29.862871Z","iopub.status.idle":"2022-03-12T08:39:31.882486Z","shell.execute_reply.started":"2022-03-12T08:39:29.862833Z","shell.execute_reply":"2022-03-12T08:39:31.88148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CPUS = 4 # kaggle default - https://www.kaggle.com/product-feedback/64106\nhappywhale_input_dir = \"/kaggle/input/happy-whale-and-dolphin\"\ntrain_csv_fn = \"train.csv\"\n# order by image jpg file and reset index to have deterministic index\ntrain_df = pd.read_csv(os.path.join(happywhale_input_dir, train_csv_fn)).sort_values('image').reset_index()\n\ntrain_df.species.replace({\"globis\": \"short_finned_pilot_whale\",\n                          \"pilot_whale\": \"short_finned_pilot_whale\",\n                          \"kiler_whale\": \"killer_whale\",\n                          \"bottlenose_dolpin\": \"bottlenose_dolphin\"}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T23:51:40.810566Z","iopub.execute_input":"2022-03-11T23:51:40.810784Z","iopub.status.idle":"2022-03-11T23:51:40.993055Z","shell.execute_reply.started":"2022-03-11T23:51:40.810758Z","shell.execute_reply":"2022-03-11T23:51:40.992279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for c in train_df.columns:\n    print('number of {}: {}'.format(c, len(train_df[c].unique())))\n","metadata":{"execution":{"iopub.status.busy":"2022-03-11T23:51:40.995233Z","iopub.execute_input":"2022-03-11T23:51:40.995466Z","iopub.status.idle":"2022-03-11T23:51:41.027345Z","shell.execute_reply.started":"2022-03-11T23:51:40.995439Z","shell.execute_reply":"2022-03-11T23:51:41.026501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from skimage.io import imread\nfrom skimage.transform import resize\nfrom skimage.color import rgb2gray, gray2rgb\nimport matplotlib.pyplot as plt\n\ndef convert_image_to_ndarray(img_path, output_size=(256,256), color='normalize_gray'):\n    '''\n    reads the `img_path` and converts it to a numpy array of size `output_size`\n    Supported Options for `color`:\n    - 'normalize_gray'\n    - 'raw_rgb'\n    \n    Returns the \n    1. normalized and standardized image\n    2. aspect ratio of the original image\n    '''\n    # read input\n    im = imread(img_path)\n    if output_size == None:\n        return im\n    aspect_ratio = float(im.shape[1] / im.shape[0])\n    resized_im = resize(im, output_size)\n    if color == 'raw_rgb':\n        return resized_im, aspect_ratio\n    elif color == 'normalize_gray':\n        # if image only has 2 channels, it's already grayscale\n        if len(resized_im.shape) == 2:\n            return resized_im / np.max(resized_im), aspect_ratio\n        # if image has 3 channels. it's rgb and needs to be converted to grayscale\n        elif len(resized_im.shape) == 3:\n            gray_img = rgb2gray(resized_im)\n            # Now normalize gray image\n            gray_norm = gray_img / np.max(gray_img)\n            return gray_norm, aspect_ratio\n        \ndef convert_ndarray_img_to_maintain_aspect_ratio(ndarray_img, ar):\n    '''\n    aspect ratio is width / height\n    '''\n    new_width = ndarray_img.shape[1]*ar # new width should be aspect ratio * img width\n    new_ar_shape = (ndarray_img.shape[0], new_width)\n    return resize(ndarray_img, new_ar_shape)\n\n\n# create lambda function to construct the full path for a given image.\n# this is intended to be used on the `image` column in train.csv\nget_img_path = lambda img: os.path.join(happywhale_input_dir, img)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T23:51:41.031027Z","iopub.execute_input":"2022-03-11T23:51:41.03129Z","iopub.status.idle":"2022-03-11T23:51:41.578574Z","shell.execute_reply.started":"2022-03-11T23:51:41.031257Z","shell.execute_reply":"2022-03-11T23:51:41.577759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://medium.com/codebyte/add-progress-bar-to-your-for-loops-5f0a50500ff3\nfrom tqdm import tqdm\n\ndef pixelize_images_in_df(df, show_progress_bar=False):\n    '''\n    Processes the dataframe from `train.csv` or `sample_submission.csv`\n    1. converting each image to a normalized grayscale (256x256) pixel representation\n    2. creating 256*256 + 1 additional features to the original dataframe\n        - each feature is 1 pixel from step 1.\n        - final feature is the aspect ratio of the original image\n    '''\n    images = df.image\n    flat_ndarray_images = []\n    aspect_ratios = []\n    n_pixels = 256\n    output_shape = (n_pixels,n_pixels)\n    items = tqdm(images) if show_progress_bar else images\n    for img in items:\n        img_path = get_img_path('train_images/' + img)\n        ndarray, ar = convert_image_to_ndarray(img_path, output_size=output_shape)\n        \n        flat_ndarray_images.append(ndarray.reshape(n_pixels*n_pixels))\n        aspect_ratios.append(ar)\n        \n    # construct final dataframe to return\n    pixelized_images_df = pd.DataFrame(flat_ndarray_images, index=df.index)\n    pixelized_images_df['aspect_ratio'] = aspect_ratios\n    return pd.merge(df, pixelized_images_df, left_index=True, right_index=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T23:51:41.579898Z","iopub.execute_input":"2022-03-11T23:51:41.580115Z","iopub.status.idle":"2022-03-11T23:51:41.587972Z","shell.execute_reply.started":"2022-03-11T23:51:41.580087Z","shell.execute_reply":"2022-03-11T23:51:41.587301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/wrosinski/parallel-data-processing-and-model-training\n\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield\n    print('{0} done in {1:.3f} seconds.'.format(name, time.time() - t0))\n\ndef split_df(df, num_splits, log=False):\n    \n    df_list = []\n    rows_splits = np.linspace(0, df.shape[0], num_splits+1).astype(int)\n    if log:\n        print('Split into {} parts'.format(num_splits))\n        print('Row splits:\\n{}'.format(rows_splits))\n    \n    for i in range(len(rows_splits) - 1):\n        df_list.append(df.iloc[rows_splits[i]:rows_splits[i+1]])\n        \n    return df_list[:num_splits]","metadata":{"execution":{"iopub.status.busy":"2022-03-11T23:51:41.589775Z","iopub.execute_input":"2022-03-11T23:51:41.59028Z","iopub.status.idle":"2022-03-11T23:51:41.605473Z","shell.execute_reply.started":"2022-03-11T23:51:41.590243Z","shell.execute_reply":"2022-03-11T23:51:41.604737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pool_process_and_dump(df, output_path_from_batch_processed, timer_str, CPUS=4, log=False):\n    df_splits = split_df(df, CPUS, log=log)\n    with timer(timer_str):\n        with Pool(processes=CPUS) as pool:\n            dfs_proc = pool.map(pixelize_images_in_df, df_splits)\n    processed_df = pd.concat(dfs_proc)\n    \n    output_dir = '/kaggle/working'\n    batch_process_dir = os.path.join(output_dir, 'batch_processed')\n    if not os.path.exists(batch_process_dir):\n        os.makedirs(batch_process_dir)\n    batch_process_fn = os.path.join(batch_process_dir,\n                            output_path_from_batch_processed)\n    processed_df.to_csv(batch_process_fn, index=False)\n    return processed_df","metadata":{"execution":{"iopub.status.busy":"2022-03-11T23:53:19.17383Z","iopub.execute_input":"2022-03-11T23:53:19.174371Z","iopub.status.idle":"2022-03-11T23:53:19.181539Z","shell.execute_reply.started":"2022-03-11T23:53:19.174331Z","shell.execute_reply":"2022-03-11T23:53:19.180748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What we want in our sampled data set\n\ntradeoff between\n- equal representation of each species with at least 1 number pictures for each individual\n- at least X number of pictures for each individual regardless of species; X becomes the tradeoff variable\n\nif we want all species and individuals,\n\nif we want all individuals in each species to have at least X images, we will not have enough images for each species. for example, the frasier_dolphine","metadata":{}},{"cell_type":"code","source":"individual_ids_by_species = pd.DataFrame(train_df[['species', 'individual_id']].value_counts()).rename({0:'count'}, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T00:57:49.527059Z","iopub.execute_input":"2022-03-12T00:57:49.528011Z","iopub.status.idle":"2022-03-12T00:57:49.575848Z","shell.execute_reply.started":"2022-03-12T00:57:49.527957Z","shell.execute_reply":"2022-03-12T00:57:49.575156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# for individual in individual_ids_by_species[individual_ids_by_species['count']>5]:\n    \nsubsetted_individuals = individual_ids_by_species[individual_ids_by_species['count']>5].reset_index()#['count'].sum()\nsubsetted_individuals#.reset_index()['individual_id']","metadata":{"execution":{"iopub.status.busy":"2022-03-12T01:04:32.835256Z","iopub.execute_input":"2022-03-12T01:04:32.83553Z","iopub.status.idle":"2022-03-12T01:04:32.852757Z","shell.execute_reply.started":"2022-03-12T01:04:32.835501Z","shell.execute_reply":"2022-03-12T01:04:32.851831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampled_data = []\nfor iid in subsetted_individuals.individual_id:\n    images_of_iid = train_df[train_df['individual_id']==iid]\n    sample = images_of_iid[:5]\n    sampled_data.append(sample)\nsampled_data_df = pd.concat(sampled_data)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T01:10:09.302923Z","iopub.execute_input":"2022-03-12T01:10:09.303218Z","iopub.status.idle":"2022-03-12T01:10:21.243208Z","shell.execute_reply.started":"2022-03-12T01:10:09.303186Z","shell.execute_reply":"2022-03-12T01:10:21.242342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampled_data_df","metadata":{"execution":{"iopub.status.busy":"2022-03-12T01:32:41.629342Z","iopub.execute_input":"2022-03-12T01:32:41.629978Z","iopub.status.idle":"2022-03-12T01:32:41.648005Z","shell.execute_reply.started":"2022-03-12T01:32:41.629925Z","shell.execute_reply":"2022-03-12T01:32:41.646811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pool_process_and_dump(sampled_data_df,\n                      'processed_batch_df_individuals_over_5_pics.csv',\n                      'pool pixelize images processing all species df')","metadata":{"execution":{"iopub.status.busy":"2022-03-12T01:14:03.402002Z","iopub.execute_input":"2022-03-12T01:14:03.402381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# min(train_df.species.value_counts())\n# Simple training set will be composed of 14 of each species with pairs of each individual id\n# all_species_training_sets = []\n# n_each_species = min(train_df.species.value_counts())\n# print('Using {} of each species'.format(n_each_species))\n# for s in train_df.species.unique():\n#     sample = train_df[train_df.species == s].head(n_each_species) # use head to be deterministic for sample\n#     all_species_training_sets.append(sample)\n    \n# all_species_train_df = pd.concat(all_species_training_sets)\n# pool_process_and_dump(all_species_train_df,\n#                       'processed_batch_df_all_species.csv',\n#                       'pool pixelize images processing all species df')\n\n# sample up to 1000 for training set\n# remaining_training_set = 1000 - len(all_species_train_df)\n# batch_size = 40\n\n# left_off_on_n = 0\n# continue_until_n = remaining_training_set # None # changing this to None will use the entire train_df\n# small_training_df = all_species_train_df.copy()\n# while left_off_on_n < continue_until_n:\n#     next_batch_index = left_off_on_n+batch_size\n    \n#     sampled_df = train_df.sample(batch_size, random_state=left_off_on_n)\n#     pool_process_and_dump(sampled_df,\n#                       'processed_batch_sampled_df_random_state_{}.csv'.format(left_off_on_n),\n#                       'pool pixelize images processing sampled batch')\n#     small_training_df = pd.concat([small_training_df, sampled_df])\n#     left_off_on_n = next_batch_index\n","metadata":{"execution":{"iopub.status.busy":"2022-03-11T23:54:20.590278Z","iopub.execute_input":"2022-03-11T23:54:20.590875Z","iopub.status.idle":"2022-03-12T00:08:54.786688Z","shell.execute_reply.started":"2022-03-11T23:54:20.590842Z","shell.execute_reply":"2022-03-12T00:08:54.785788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# len(small_training_df['individual_id'].unique()), small_training_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-12T00:13:43.08208Z","iopub.execute_input":"2022-03-12T00:13:43.083034Z","iopub.status.idle":"2022-03-12T00:13:43.093549Z","shell.execute_reply.started":"2022-03-12T00:13:43.082984Z","shell.execute_reply":"2022-03-12T00:13:43.092668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-03-12T00:13:47.711608Z","iopub.execute_input":"2022-03-12T00:13:47.712258Z","iopub.status.idle":"2022-03-12T00:13:47.717003Z","shell.execute_reply.started":"2022-03-12T00:13:47.712209Z","shell.execute_reply":"2022-03-12T00:13:47.716148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# x_cols = []\n\n# X_train, X_test, y_train, y_test = train_test_split(small_training_df[x_cols], small_training_df['individual_id'], test_size=.2, random_state=2022)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # building a linear stack of layers with the sequential model\n# model = Sequential()\n# # convolutional layer\n# model.add(Conv2D(25, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu', input_shape=(28,28,1)))\n# model.add(MaxPool2D(pool_size=(1,1)))\n# # flatten output of conv\n# model.add(Flatten())\n# # hidden layer\n# model.add(Dense(100, activation='relu'))\n# # output layer\n# model.add(Dense(10, activation='softmax'))\n\n# # compiling the sequential model\n# model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n\n# # training the model for 10 epochs\n# model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_test, Y_test))","metadata":{},"execution_count":null,"outputs":[]}]}